% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/00_S7_init.R, R/train.R
\name{train}
\alias{train}
\alias{train.class_tabular}
\title{Train Supervised Learning Models}
\usage{
train(x, ...)

train.class_tabular(
  x,
  dat_validation = NULL,
  dat_test = NULL,
  weights = NULL,
  algorithm = NULL,
  preprocessor_config = NULL,
  hyperparameters = NULL,
  tuner_config = NULL,
  outer_resampling_config = NULL,
  execution_config = setup_ExecutionConfig(),
  question = NULL,
  outdir = NULL,
  verbosity = 1L,
  ...
)
}
\arguments{
\item{x}{tabular data, i.e. data.frame, data.table, or tbl_df (tibble): Training set data.}

\item{...}{Not used.}

\item{dat_validation}{tabular data: Validation set data.}

\item{dat_test}{tabular data: Test set data.}

\item{weights}{Optional vector of case weights.}

\item{algorithm}{Character: Algorithm to use. Can be left NULL, if \code{hyperparameters} is defined.}

\item{preprocessor_config}{PreprocessorConfig object or NULL: Setup using \link{setup_Preprocessor}.}

\item{hyperparameters}{\code{Hyperparameters} object: Setup using one of \verb{setup_*} functions.}

\item{tuner_config}{TunerConfig object: Setup using \link{setup_GridSearch}.}

\item{outer_resampling_config}{ResamplerConfig object or NULL: Setup using \link{setup_Resampler}. This
defines the outer resampling method, i.e. the splitting into training and test sets for the
purpose of assessing model performance. If NULL, no outer resampling is performed, in which case
you might want to use a \code{dat_test} dataset to assess model performance on a single test set.}

\item{execution_config}{\code{ExecutionConfig} object: Setup using \link{setup_ExecutionConfig}. This
allows you to set backend ("future", "mirai", or "none"), number of workers, and future plan if
using \code{backend = "future"}.}

\item{question}{Optional character string defining the question that the model is trying to
answer.}

\item{outdir}{Character, optional: String defining the output directory.}

\item{verbosity}{Integer: Verbosity level.}
}
\value{
Object of class \code{Regression(Supervised)}, \code{RegressionRes(SupervisedRes)},
\code{Classification(Supervised)}, or \code{ClassificationRes(SupervisedRes)}.
}
\description{
Preprocess, tune, train, and test supervised learning models in a single call
using nested resampling.
}
\details{
\strong{Online documentation}

See \href{https://rdocs.rtemis.org/train}{rdocs.rtemis.org/train} for detailed documentation.

\strong{Binary Classification}

For binary classification, the outcome should be a factor where the 2nd level
corresponds to the positive class.

\strong{Resampling}

Note that you should not use an outer resampling method with
replacement if you will also be using an inner resampling (for tuning).
The duplicated cases from the outer resampling may appear both in the
training and test sets of the inner resamples, leading to underestimated
test error.

\strong{Reproducibility}

If using \emph{\strong{outer resampling}}, you can set a seed when defining \code{outer_resampling_config}, e.g.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{outer_resampling_config = setup_Resampler(n_resamples = 10L, type = "KFold", seed = 2026L)
}\if{html}{\out{</div>}}

If using \emph{\strong{tuning with inner resampling}}, you can set a seed when defining \code{tuner_config},
e.g.

\if{html}{\out{<div class="sourceCode r">}}\preformatted{tuner_config = setup_GridSearch(
  resampler_config = setup_Resampler(n_resamples = 5L, type = "KFold", seed = 2027L)
)
}\if{html}{\out{</div>}}

\strong{Parallelization}

There are three levels of parallelization that may be used during training:
\enumerate{
\item Algorithm training (e.g. a parallelized learner like LightGBM)
\item Tuning (inner resampling, where multiple resamples can be processed in parallel)
\item Outer resampling (where multiple outer resamples can be processed in parallel)
}

The \code{train()} function will automatically manage parallelization depending
on:
\itemize{
\item The number of workers specified by the user using \code{n_workers}
\item Whether the training algorithm supports parallelization itself
\item Whether hyperparameter tuning is needed
}
}
\examples{
\donttest{
iris_c_lightRF <- train(
   iris,
   algorithm = "LightRF",
   outer_resampling_config = setup_Resampler(),
)
}
}
\author{
EDG
}
